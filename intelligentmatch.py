# -*- coding: utf-8 -*-
"""intelligentMatch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OnrfoA2TkQya-NORGRO9_hBd-H80sVL0

#(M22RM007- Soham Padhye)
# Use Image Registration with SIFT to find the matching points in the images
#In all codes written below first image is probe image and second image is reference image
"""

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

def find_and_apply_homography(ref_img, input_img):

    # Find the keypoints and descriptors with SIFT
    sift = cv2.SIFT_create()
    ref_kp, ref_des = sift.detectAndCompute(ref_img, None)
    input_kp, input_des = sift.detectAndCompute(input_img, None)

    # Match the keypoints using FLANN Matcher
    #Set the Tree,k,checks parameters accurately to get good resuls
    FLANN_INDEX_KDTREE = 1
    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=50)
    search_params = dict(checks=50)
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matches = flann.knnMatch(ref_des, input_des, k=2)

    # Apply ratio test to filter good matches
    good_matches = []
    for m,n in matches:
        #If the distance between descriptors is more then ignore it
        if m.distance < 0.8 * n.distance:
            good_matches.append(m)

    # Extract the matched keypoints
    src_pts = np.float32([ref_kp[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([input_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

    # Calculate the homography matrix using inbuilt function
    H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 10.0)

    # Apply the homography matrix to the input image to get registered image
    result = cv2.warpPerspective(input_img, H, (ref_img.shape[1], ref_img.shape[0]))

    # Display the result
    cv2_imshow(result)
    return result

"""# Image after registration"""

#Example 1-faulty image
# Load the reference and input images
ref_img = cv2.imread('reference.png', 1)
input_img = cv2.imread('Faulty_2.png', 1)
result=find_and_apply_homography(ref_img,input_img)

"""# Find difference between registered and reference image"""

img3=cv2.absdiff(result,ref_img)
cv2_imshow(img3)

"""# Use ORB to find the corrospondance between the images and then use Image Registration"""

import cv2
import numpy as np

def find_and_apply_homography(ref_img, input_img):
    

    # Find the keypoints and descriptors with ORB
    orb = cv2.ORB_create()
    ref_kp, ref_des = orb.detectAndCompute(ref_img, None)
    input_kp, input_des = orb.detectAndCompute(input_img, None)

    # Match the keypoints using FLANN Matcher
    FLANN_INDEX_LSH = 6
    index_params= dict(algorithm = FLANN_INDEX_LSH,
                        table_number = 6, 
                        key_size = 12,     
                        multi_probe_level = 1)
    search_params = dict(checks=50)
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matches = flann.knnMatch(ref_des, input_des, k=2)

    # Apply ratio test to filter good matches
    good_matches = []
    for m,n in matches:
        if m.distance < 0.8 * n.distance:
            good_matches.append(m)

    # Extract the matched keypoints
    src_pts = np.float32([ref_kp[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([input_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

    # Calculate the homography matrix
    H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 10.0)

    # Apply the homography matrix to the input image
    result = cv2.warpPerspective(input_img, H, (ref_img.shape[1], ref_img.shape[0]))

    # Display the result
    cv2_imshow(result)
    return result

"""# Registered Image using ORB"""

# Load the reference and input images
ref_img = cv2.imread('reference.png', 1)
input_img = cv2.imread('Faulty_2.png', 1)
result=find_and_apply_homography(ref_img,input_img)

img3=cv2.absdiff(result,ref_img)
cv2_imshow(img3)

"""# Use ORB to find Keypoints and showing the unmatched region in faulty image"""

import cv2
import numpy as np

def detect_unmatched_keypoints(img1, img2):
    # Convert the images to grayscale
    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
    
    # Create an ORB detector
    orb = cv2.ORB_create()

    # Detect keypoints and compute descriptors for both images
    kp1, des1 = orb.detectAndCompute(gray1, None)
    kp2, des2 = orb.detectAndCompute(gray2, None)
    
    # Match keypoints between the two images using a brute-force matcher
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    matches = bf.match(des1, des2)

    # Get all the match distances which are useful in further calculation
    match_distances = [match.distance for match in matches]

    # Sort the matches by distance
    matches = sorted(matches, key=lambda x: x.distance)
    
    # Create an empty list to store match distances
    distances = []

    for match in matches:
        # Append the distance of each match to the list
        distances.append(match.distance)
    # Print this to get the idea about the distace between the descriptors.On this distance value
    #we are applying the bad matches condition
    print(distances)
    
    for match in matches:
        # Draw rectangles around the keypoints in image 1 that do not have a match in image 2
        if match.distance > 62:#get this threshold value by seeing the distances  between descriptors that are printed below
        #If the destance between the descriptors is more then corredpondingkeypoint
        #is not matching means it shows the unmatched region in the image
            pt1 = (int(kp1[match.queryIdx].pt[0]), int(kp1[match.queryIdx].pt[1]))
            #Draw rectangele around the defective region
            rect_size = 30
            rect_pt1 = (int(pt1[0] - rect_size / 2), int(pt1[1] - rect_size / 2))
            rect_pt2 = (int(pt1[0] + rect_size / 2), int(pt1[1] + rect_size / 2))
            cv2.rectangle(img1, rect_pt1, rect_pt2, (0, 0, 0), 2)
    
    # Display the images with the unmatched keypoints
    cv2_imshow(img1)
    cv2_imshow(img2)

# Load the images
img1 = cv2.imread('Faulty_1_1.png')
img2 = cv2.imread('reference_1.png')
detect_unmatched_keypoints(img1, img2)

# Load the images
img1 = cv2.imread('Faulty_1_2.png')
img2 = cv2.imread('reference_1.png')
detect_unmatched_keypoints(img1, img2)

# Load the images
img1 = cv2.imread('perfect_1_1.png')
img2 = cv2.imread('reference_1.png')
detect_unmatched_keypoints(img1, img2)

# Load the images
img1 = cv2.imread('perfect_1_2.png')
img2 = cv2.imread('reference_1.png')
detect_unmatched_keypoints(img1, img2)

# Load the images
img1 = cv2.imread('Faulty_2_1.png')
img2 = cv2.imread('reference_2.png')
detect_unmatched_keypoints(img1, img2)

# Load the images
img1 = cv2.imread('Faulty_2_2.png')
img2 = cv2.imread('reference_2.png')
detect_unmatched_keypoints(img1, img2)

# Load the images
img1 = cv2.imread('perfect_2_1.png')
img2 = cv2.imread('reference_2.png')
detect_unmatched_keypoints(img1, img2)

# Load the images
img1 = cv2.imread('perfect_2_2.png')
img2 = cv2.imread('reference_2.png')
detect_unmatched_keypoints(img1, img2)

"""# Use SIFT to find keypoints and find the difference between reference image and faulty image by non matching keypoints"""

def SIFT_detect_unmatched_keypoints(img1,img2,threshold):
    #Threshold value is chosen in such a way that only bad matches will be selected
    # Convert the images to grayscale
    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

    # Create a SIFT detector
    sift = cv2.SIFT_create()

    # Detect keypoints and compute descriptors for both images
    kp1, des1 = sift.detectAndCompute(gray1, None)
    kp2, des2 = sift.detectAndCompute(gray2, None)

    # Match keypoints between the two images using a brute-force matcher
    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
    matches = bf.match(des1, des2)

    # Get all the match distances
    match_distances = [match.distance for match in matches]

    # Sort the matches by distance
    matches = sorted(matches, key=lambda x: x.distance)

    # Create an empty list to store match distances
    distances = []

    for match in matches:
        # Append the distance of each match to the list
        distances.append(match.distance)
    #Printing the distance to set the threshold value by trial and error
    print(distances)

    for match in matches:
        # Draw rectangles around the keypoints in image 1 that do not have a match in image 2
        if match.distance > threshold:#get this threshold value by seeing the distances  between descriptors that are printed below
        #If the destance between the descriptors is more then corredpondingkeypoint
        #is not matching means it shows the unmatched region in the image
            pt1 = (int(kp1[match.queryIdx].pt[0]), int(kp1[match.queryIdx].pt[1]))
            #Draw the rectangle for defect in the image
            rect_size = 30
            rect_pt1 = (int(pt1[0] - rect_size / 2), int(pt1[1] - rect_size / 2))
            rect_pt2 = (int(pt1[0] + rect_size / 2), int(pt1[1] + rect_size / 2))
            cv2.rectangle(img1, rect_pt1, rect_pt2, (0, 0, 0), 2)

    # Display the image with the unmatched keypoints
    cv2_imshow(img1)
    cv2_imshow(img2)

# Load the images
img1 = cv2.imread('Faulty_1_1.png')
img2 = cv2.imread('reference_1.png')
SIFT_detect_unmatched_keypoints(img1,img2,310)

# Load the images
img1 = cv2.imread('Faulty_1_2.png')
img2 = cv2.imread('reference_1.png')
SIFT_detect_unmatched_keypoints(img1,img2,310)

# Load the images
img1 = cv2.imread('perfect_1_1.png')
img2 = cv2.imread('reference_1.png')
SIFT_detect_unmatched_keypoints(img1,img2,310)

# Load the images
img1 = cv2.imread('perfect_1_2.png')
img2 = cv2.imread('reference_1.png')
SIFT_detect_unmatched_keypoints(img1,img2,310)

# Load the images
img1 = cv2.imread('Faulty_2_1.png')
img2 = cv2.imread('reference_2.png')
SIFT_detect_unmatched_keypoints(img1,img2,290)

# Load the images
img1 = cv2.imread('Faulty_2_2.png')
img2 = cv2.imread('reference_2.png')
SIFT_detect_unmatched_keypoints(img1,img2,290)

# Load the images
img1 = cv2.imread('perfect_2_1.png')
img2 = cv2.imread('reference_2.png')
SIFT_detect_unmatched_keypoints(img1,img2,290)

# Load the images
img1 = cv2.imread('perfect_2_2.png')
img2 = cv2.imread('reference_2.png')
SIFT_detect_unmatched_keypoints(img1,img2,290)